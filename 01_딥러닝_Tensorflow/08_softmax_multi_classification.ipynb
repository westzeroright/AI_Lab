{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6540cc65",
   "metadata": {},
   "source": [
    "### multi-classification\n",
    ": multi-nomial classification (다중 분류) : Y값의 범주가 3개 이상인 분류\n",
    "#### 활성화 함수(Activation function) 으로 softmax함수 가 사용된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9100f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.random.set_seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ea01f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data set :\n",
    "# x_data :  [N,4]  --> [8,4]\n",
    "x_data = [[1,2,1,1],\n",
    "          [2,1,3,2],\n",
    "          [3,1,3,4],\n",
    "          [4,1,5,5],\n",
    "          [1,7,5,5],\n",
    "          [1,2,5,6],\n",
    "          [1,6,6,6],\n",
    "          [1,7,7,7]]\n",
    "\n",
    "# y_data : [N,3] --> [8,3]\n",
    "y_data = [[0,0,1],  # [2]\n",
    "          [0,0,1],  # [2]\n",
    "          [0,0,1],  # [2]\n",
    "          [0,1,0],  # [1]\n",
    "          [0,1,0],  # [1]\n",
    "          [0,1,0],  # [1]\n",
    "          [1,0,0],  # [0]\n",
    "          [1,0,0]]  # [0]\n",
    "\n",
    "x_train = np.array(x_data,dtype=np.float32)\n",
    "y_train = np.array(y_data,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bcfd7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'weight:0' shape=(4, 3) dtype=float32, numpy=\n",
      "array([[-0.18030666, -0.95028627, -0.03964049],\n",
      "       [-0.7425406 ,  1.3231523 , -0.61854804],\n",
      "       [ 0.8540664 , -0.08899953,  2.4488697 ],\n",
      "       [ 0.762508  ,  1.2659615 ,  0.9801489 ]], dtype=float32)>\n",
      "<tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([0.22652863, 0.8106553 , 0.7466094 ], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "nb_classes = 3\n",
    "# 변수 초기화 : weight, bias\n",
    "# (m,n) * (n,l) = (m,l)   : 행렬의 내적 곱셈 공식\n",
    "# (8,4) * (4,3) = (8,3)\n",
    "W = tf.Variable(tf.random.normal([4,nb_classes]),name='weight')\n",
    "b = tf.Variable(tf.random.normal([nb_classes]),name='bias')\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a53b3399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 함수(hypothesis) : H(X) = softmax(X*W + B)\n",
    "def logits(X):\n",
    "    return tf.matmul(X,W) + b\n",
    "\n",
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(logits(X))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e17fd4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  비용 함수 구현 방법 1: log함수를 사용하여 수식을 직접 표현\n",
    "# def cost_func():\n",
    "#     cost = tf.reduce_mean(-tf.reduce_sum(y_train*tf.math.log(hypothesis(x_train)),\n",
    "#                                          axis=1))\n",
    "#     return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e47ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  비용함수 구현 방법 2 : tf.nn.softmax_cross_entropy_with_logits() 함수 사용\n",
    "def cost_func():\n",
    "    cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits(x_train),\n",
    "                                                    labels=y_train)\n",
    "    cost = tf.reduce_mean(cost_i)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bef8c0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사 하강법\n",
    "# learning_rate(학습율)을 0.01 로 설정하여 optimizer객체를 생성\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97afa60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Start Learning!!\n",
      "0000 cost:[ 5.9294786 ]  W: [[-0.17030679 -0.94028634 -0.04964045]\n",
      " [-0.7325406   1.3331522  -0.628548  ]\n",
      " [ 0.86406636 -0.07899956  2.4388697 ]\n",
      " [ 0.77250797  1.2759615   0.9701489 ]]  b: [0.2365285  0.8206551  0.73660946]\n",
      "1000 cost:[ 0.314676 ]  W: [[-2.2194262e+00  5.2480602e-01  1.1872499e+00]\n",
      " [ 1.2104913e-01  2.2530544e-03 -6.3700818e-02]\n",
      " [ 2.2870729e+00  1.1651007e+00  6.7435008e-01]\n",
      " [ 1.2760863e+00  1.7768888e+00  3.7673792e-01]]  b: [-2.7187707 -1.5067681  4.3172646]\n",
      "2000 cost:[ 0.16304696 ]  W: [[-3.7972472   1.3537483   2.2676582 ]\n",
      " [ 0.04149133  0.02373761  0.06498493]\n",
      " [ 3.5885081   1.049066   -0.5021988 ]\n",
      " [ 1.0556881   1.8170282   0.5562775 ]]  b: [-6.1154466 -1.3375355  7.1691594]\n",
      "3000 cost:[ 0.08965948 ]  W: [[-5.234304    2.1662517   3.2086873 ]\n",
      " [-0.04390996  0.06965788  0.1601938 ]\n",
      " [ 4.8728323   0.69199103 -1.575506  ]\n",
      " [ 0.7372118   2.0572197   0.74264187]]  b: [-9.075829 -1.360418  9.762421]\n",
      "4000 cost:[ 0.051186908 ]  W: [[-6.5691333   2.9417892   4.072175  ]\n",
      " [-0.1296271   0.12063979  0.24656796]\n",
      " [ 6.1185436   0.27676716 -2.5946252 ]\n",
      " [ 0.37533334  2.3316693   0.9539057 ]]  b: [-11.7408495  -1.3668214  12.113412 ]\n",
      "5000 cost:[ 0.029821247 ]  W: [[-7.8441849e+00  3.6925678e+00  4.8953743e+00]\n",
      " [-2.1366496e-01  1.7167549e-01  3.2927576e-01]\n",
      " [ 7.3285356e+00 -1.4589591e-01 -3.5808389e+00]\n",
      " [-2.5742422e-03  2.6041067e+00  1.1803541e+00]]  b: [-14.22405   -1.346219  14.310619]\n",
      "***** Learning Finished!!\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "print('***** Start Learning!!')\n",
    "for step in range(5001):\n",
    "    optimizer.minimize(cost_func,var_list=[W,b])\n",
    "    if step % 1000 == 0:\n",
    "        print('%04d'%step,'cost:[',cost_func().numpy(),']',\n",
    "             ' W:',W.numpy(),' b:',b.numpy())\n",
    "print('***** Learning Finished!!') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38d233f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 1 1 1 0 0]\n",
      "[[6.7201379e-15 4.4788053e-06 9.9999547e-01]\n",
      " [3.0885360e-11 6.2573748e-03 9.9374264e-01]\n",
      " [8.2859493e-18 3.1583805e-02 9.6841621e-01]\n",
      " [5.7242958e-16 9.7510344e-01 2.4896551e-02]\n",
      " [5.8690526e-02 9.3963611e-01 1.6734067e-03]\n",
      " [3.0667935e-02 9.6914309e-01 1.8900653e-04]\n",
      " [9.2271829e-01 7.7280879e-02 9.1235523e-07]\n",
      " [9.9905401e-01 9.4603549e-04 1.0147042e-10]]\n",
      "[2 2 2 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# 예측\n",
    "# tf.argmax() : 값이 가장 큰 요소의 인덱스 값을 반환\n",
    "def predict(X):\n",
    "    return tf.argmax(hypothesis(X),axis=1)\n",
    "\n",
    "# 학습 데이터를 검증 데이터로 동일하게 사용하는 경우\n",
    "x_test = x_train\n",
    "y_test = y_train\n",
    "\n",
    "preds = predict(x_test)\n",
    "print(preds.numpy())\n",
    "print(hypothesis(x_test).numpy())\n",
    "print(tf.argmax(y_test,axis=1).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
